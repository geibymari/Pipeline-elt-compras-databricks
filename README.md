# Pipeline ELT Compras Databricks ğŸª

> Pipeline **ELT end-to-end** para procesamiento de Ã³rdenes de compra (Presencial y Online) usando **Databricks** y **Delta Lake** con arquitectura escalable y validaciones de calidad.

[![Python](https://img.shields.io/badge/Python-3.9+-blue?logo=python)](https://www.python.org/)
[![Databricks](https://img.shields.io/badge/Databricks-Latest-red?logo=databricks)](https://databricks.com/)
[![Delta Lake](https://img.shields.io/badge/Delta%20Lake-Enabled-green)](https://delta.io/)
[![Status](https://img.shields.io/badge/Status-Production%20Ready-brightgreen)]()
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## ğŸ“‹ Tabla de Contenidos

- [DescripciÃ³n del Proyecto](#descripciÃ³n-del-proyecto)
- [Arquitectura ELT](#arquitectura-elt)
- [TecnologÃ­as](#tecnologÃ­as)
- [Requisitos](#requisitos)
- [InstalaciÃ³n](#instalaciÃ³n)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [Notebooks Principales](#notebooks-principales)
- [Flujo de Datos](#flujo-de-datos)
- [Validaciones de Calidad](#validaciones-de-calidad)
- [AutomatizaciÃ³n con Jobs](#automatizaciÃ³n-con-jobs)
- [Resultados & MÃ©tricas](#resultados--mÃ©tricas)
- [Troubleshooting](#troubleshooting)
- [Contacto](#contacto)

---

## ğŸ“Œ DescripciÃ³n del Proyecto

Este proyecto implementa un **pipeline ELT completo** que procesa datos de compras desde mÃºltiples canales (Presencial y Online) mediante una arquitectura de 3 capas en Databricks:

âœ… **Extrae** datos de CSV (compras presencial) y JSON (compras online)  
âœ… **Carga** en capa Bronze con trazabilidad completa  
âœ… **Transforma** con limpieza, validaciÃ³n y enriquecimiento de datos  
âœ… **Valida** calidad en cada capa con reglas automÃ¡ticas  
âœ… **Genera** tablas analÃ­ticas en capa Gold con joins y agregaciones  
âœ… **Automatiza** mediante Jobs con notificaciones por email  

### ğŸ¯ Caso de Uso

Procesamiento de **Ã³rdenes de compra** de una plataforma de e-commerce (estilo Linio) que captura:
- **Compras Presencial**: Tiendas fÃ­sicas con datos en CSV
- **Compras Online**: Plataforma digital con datos en JSON
- **Detalles de Compra**: Productos, categorÃ­as, precios (mÃºltiples CSV)

Resultado: Tabla de hechos analÃ­tica (`gold_fact_compras`) con informaciÃ³n consolidada para BI/reportes.

---

## ğŸ—ï¸ Arquitectura ELT

<img width="2400" height="1600" alt="image" src="https://github.com/user-attachments/assets/b9fb05da-d3f4-4036-a7d8-cb54176c4130" />

```

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FUENTES DE DATOS                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Presencial.csv     â”‚  â”‚  Online.json + Detalles/*.csv    â”‚  â”‚
â”‚  â”‚  (Tiendas fÃ­sicas)  â”‚  â”‚  (Plataforma Online)             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ğŸŸ« CAPA BRONZE (Raw)                         â”‚
â”‚  01_bronze_layer â†’ bronze_compras, bronze_detalles              â”‚
â”‚  - Ingesta sin transformaciÃ³n (solo strings)                     â”‚
â”‚  - Renombre de columnas (snake_case)                             â”‚
â”‚  - Trazabilidad (tipo_compra, fecha_carga)                       â”‚
â”‚  01_bronze_calidad â†’ log_calidad_datos                           â”‚
â”‚  - Validaciones: no nulos, duplicados, formato                   â”‚
â”‚  - ParÃ¡metros de control â†’ estado, detalle (JSON)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ğŸŸ© CAPA SILVER (Clean)                       â”‚
â”‚  02_silver_layer â†’ silver_compras, silver_detalles              â”‚
â”‚  - ConversiÃ³n de tipos de datos                                  â”‚
â”‚  - Limpieza de espacios y formato de texto                       â”‚
â”‚  - Transformaciones complejas:                                   â”‚
â”‚    â€¢ Estado (1â†’"Creado", 2â†’"En Curso", etc.)                   â”‚
â”‚    â€¢ ExtracciÃ³n cliente_id + num_documento                      â”‚
â”‚    â€¢ Tipo de documento (DNI, RUC10, RUC20)                      â”‚
â”‚    â€¢ Nombre cliente (concat nombres + apellidos)                â”‚
â”‚    â€¢ DÃ­as abierto + grupo de dÃ­as                               â”‚
â”‚  02_silver_calidad â†’ log_calidad_datos                           â”‚
â”‚  - Validaciones: relaciones entre fechas, rangos                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ğŸŸ¨ CAPA GOLD (Analytics)                     â”‚
â”‚  03_gold_layer â†’ gold_fact_compras                               â”‚
â”‚  - Join silver_compras + silver_detalles (match por factura)    â”‚
â”‚  - Tabla de hechos con todas las dimensiones                     â”‚
â”‚  - Listo para BI (Power BI, Tableau, Looker)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ğŸ“Š VISUALIZACIÃ“N & BI                          â”‚
â”‚  Power BI | Tableau | Databricks SQL Analytics                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ TecnologÃ­as

| Componente | Herramienta | VersiÃ³n | Detalle |
|-----------|-----------|---------|---------|
| **Plataforma** | Databricks | Latest | Enterprise/Community |
| **Motor de Datos** | Apache Spark | 3.4.x | Distributed Computing |
| **Storage** | Delta Lake | Latest | ACID Transactions |
| **Cloud** | Microsoft Azure | - | Infrastructure |
| **Lenguaje** | Python | 3.9+ | PySpark |
| **OrquestaciÃ³n** | Databricks Workflows | - | Jobs & Tasks |
| **Notificaciones** | Email Alerts | - | Success/Error/Quality |
| **Testing** | pytest | Latest | Unit & Integration |
| **BI** | Power BI / Tableau | Latest | VisualizaciÃ³n |

---

## ğŸ“¦ Requisitos Previos

- âœ… Cuenta activa en **Databricks** (Community o Enterprise)
- âœ… Acceso a **Azure Storage** o almacenamiento local
- âœ… Datasets descargados (`dataset.zip` con archivos CSV/JSON)
- âœ… Notebooks de email descargados (`Correos.dbc`)
- âœ… Python 3.9+
- âœ… Git
- âœ… pip (gestor de paquetes)

---

## ğŸš€ InstalaciÃ³n & ConfiguraciÃ³n

### 1ï¸âƒ£ PreparaciÃ³n en Databricks

```bash
# A) Crear esquema en Databricks
CREATE SCHEMA linio;

# B) Crear volumes para datos
CREATE VOLUME linio.compras;
CREATE VOLUME linio.detalles;

# C) Cargar archivos del dataset.zip
# - Presencial.csv â†’ /Volumes/linio/compras/
# - Online.json â†’ /Volumes/linio/compras/
# - *.csv (detalles) â†’ /Volumes/linio/detalles/

# D) Importar notebooks de email
# Descargar Correos.dbc â†’ Import en Databricks
```

### 2ï¸âƒ£ Clonar el Repositorio

```bash
git clone https://github.com/tu-usuario/elt-compras-databricks.git
cd elt-compras-databricks
```

### 3ï¸âƒ£ Instalar Dependencias (Local)

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 4ï¸âƒ£ Configurar Credenciales

Crea `.env` en la raÃ­z:

```env
# Databricks
DATABRICKS_HOST=https://your-workspace.cloud.databricks.com
DATABRICKS_TOKEN=dapiXXXXXXXXXXXXXXXX

# Proyecto ELT
SCHEMA_NAME=linio
VOLUME_COMPRAS=linio.compras
VOLUME_DETALLES=linio.detalles

# Email notifications
EMAIL_FROM=your-email@example.com
EMAIL_SMTP=smtp.gmail.com
EMAIL_PORT=587
EMAIL_USER=your-email@example.com
EMAIL_PASSWORD=your-app-password
EMAIL_TO=destinatario@example.com
```

### 5ï¸âƒ£ En Databricks - Importar Notebooks

OpciÃ³n A: Manualmente
```
Workspace â†’ Import â†’ Seleccionar archivos de /notebooks
```

OpciÃ³n B: CLI
```bash
databricks workspace import_dir ./notebooks /Users/tu-usuario/elt-compras
```

---

## ğŸ“ Estructura del Proyecto

```
pipeline-elt-compras-databricks/
â”œâ”€â”€ ğŸ“ notebooks/
â”‚   â”œâ”€â”€ ğŸ“ bronze_layer/
â”‚   â”‚   â”œâ”€â”€ 01_bronze_layer.ipynb          # IngestiÃ³n de datos raw
â”‚   â”‚   â””â”€â”€ 01_bronze_calidad.ipynb        # ValidaciÃ³n calidad Bronze
â”‚   â”œâ”€â”€ ğŸ“ silver_layer/
â”‚   â”‚   â”œâ”€â”€ 02_silver_layer.ipynb          # Transformaciones y limpieza
â”‚   â”‚   â””â”€â”€ 02_silver_calidad.ipynb        # ValidaciÃ³n calidad Silver
â”‚   â”œâ”€â”€ ğŸ“ gold_layer/
â”‚   â”‚   â””â”€â”€ 03_gold_layer.ipynb            # ConsolidaciÃ³n analytics
â”‚   â””â”€â”€ ğŸ“ notifications/
â”‚       â”œâ”€â”€ enviar_correo_exitoso.ipynb    # NotificaciÃ³n Ã©xito
â”‚       â”œâ”€â”€ enviar_correo_error.ipynb      # NotificaciÃ³n error
â”‚       â””â”€â”€ enviar_correo_calidad.ipynb    # NotificaciÃ³n calidad
â”œâ”€â”€ ğŸ“ data/
â”‚   â”œâ”€â”€ ğŸ“ raw/
â”‚       â”œâ”€â”€ ğŸ“ compras/
â”‚       â”‚   â”œâ”€â”€ Presencial.csv             # Compras presenciales
â”‚       â”‚   â””â”€â”€ Online.json                # Compras online
â”‚       â””â”€â”€ ğŸ“ detalles/
â”‚           â”œâ”€â”€ detalle_001.csv
â”‚           â”œâ”€â”€ detalle_002.csv
â”‚           â””â”€â”€ ...
â”œâ”€â”€ ğŸ“ docs/
â”‚   â””â”€â”€ architecture_diagram.png
â”‚ 
â”œâ”€â”€ ğŸ“ assets/
â”‚   â””â”€â”€ ğŸ“ screenshots/
â”‚       â”œâ”€â”€ databricks_job.png
â”‚       â””â”€â”€ delta_tables.png
â”‚  
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## ğŸ“– Notebooks Principales

### ğŸŸ« Bronze Layer

#### `01_bronze_layer.ipynb`
**Objetivo**: Ingesta raw de datos sin transformaciÃ³n

```python
# Flujo:
1. Leer Presencial.csv â†’ df_compras_presencial (strings)
2. Renombrar columnas a snake_case
3. Agregar tipo_compra="Presencial", fecha_carga=now()
4. Leer Online.json â†’ df_compras_online (strings con StructType)
5. Renombrar columnas a snake_case
6. Agregar tipo_compra="Online", fecha_carga=now()
7. UNION df_compras_presencial + df_compras_online â†’ df_compras
8. Leer masivamente *.csv de detalles â†’ df_detalles
9. Renombrar columnas, agregar nombre_archivo, fecha_carga
10. Crear/poblar tablas delta:
    - bronze_compras (con try...except)
    - bronze_detalles (con try...except)
```

**Entradas**: CSV presencial, JSON online, CSV detalles (volumes)  
**Salidas**: `bronze_compras`, `bronze_detalles` tables  

---

#### `01_bronze_calidad.ipynb`
**Objetivo**: Validar calidad de datos en capa bronze

```python
# Reglas para bronze_compras:
- [factura] â‰  null
- [factura] â‰¥ 7 caracteres
- [factura] sin duplicados
- [fecha_orden] â‰  null

# Reglas para bronze_detalles:
- [factura] â‰  null
- [producto] â‰  null

# Salida: log_calidad_datos (append mode)
# ParÃ¡metros exportados al siguiente notebook:
- estado: "Validacion Exitosa" | "Falla Critica"
- detalle: JSON con registros fallidos (si aplica)
```

---

### ğŸŸ© Silver Layer

#### `02_silver_layer.ipynb`
**Objetivo**: TransformaciÃ³n, limpieza y enriquecimiento

**ParÃ¡metro**: `fecha_carga` (default: '2025-06-16')

```python
# SILVER_COMPRAS - Transformaciones:
- venta_id, estado â†’ integer
- fecha_orden, fecha_entrega, fecha_envio â†’ date
- factura, tipo_cliente â†’ UPPER + TRIM
- nombres, apellidos â†’ INITCAP + TRIM
- vendedor, departamento, metodo_pago â†’ TRIM
- estado: 1â†’"Creado", 2â†’"En Curso", 3â†’"Programado", 4â†’"Cancelado", 5â†’"Entregado"
- cliente_id: split por "-" â†’ cliente_id (int), num_documento (string)
- num_documento: pad left con 0 si < 8 chars
- tipo_documento: "DNI" (8 chars) | "RUC10" (11, inicia con 10) | "RUC20" (11, inicia con 20)
- nombre_cliente: concat(nombres, apellidos)
- dias_abierto: DATEDIFF(fecha_carga, fecha_orden) si estado IN ("Creado", "En Curso", "Programado")
- grupo_dias_abierto: "[0-3 dÃ­as]" | "[4-7 dÃ­as]" | "[mÃ¡s de 8 dÃ­as]"

# SILVER_DETALLES - Transformaciones:
- detalle_id, unidades, oferta_id â†’ integer
- precio_unitario â†’ double
- factura â†’ UPPER + TRIM
- categoria, subcategoria, producto â†’ TRIM
- subtotal: unidades * precio_unitario
- tienda: extract antes del "." en nombre_archivo
```

---

#### `02_silver_calidad.ipynb`
**Objetivo**: Validar calidad despuÃ©s de transformaciones

```python
# Reglas para silver_compras:
- Si [estado]="Entregado" â†’ [fecha_envio] â‰  null
- Si [estado]="Entregado" â†’ [fecha_envio] â‰¥ [fecha_orden]

# Reglas para silver_detalles:
- [subtotal] > 0

# Salida: log_calidad_datos (append mode)
# ParÃ¡metros: estado, detalle (si aplica)
```

---

### ğŸŸ¨ Gold Layer

#### `03_gold_layer.ipynb`
**Objetivo**: Crear tabla de hechos analÃ­tica

```python
# Flujo:
1. Leer silver_compras, silver_detalles
2. INNER JOIN por [factura]
3. Seleccionar columnas finales (20+ columnas)
4. Agregar fecha_carga (trazabilidad)
5. Crear/poblar gold_fact_compras (con try...except)

# Tabla resultante:
gold_fact_compras (1 fila por detalle de compra, con info de orden)
Ideal para: dashboards, reportes, anÃ¡lisis OLAP
```

---

## ğŸ”„ Flujo de Datos Completo

```
EJECUCIÃ“N MANUAL (para testing):
1. Run 00_setup.ipynb (crear schema + volumes)
2. Run 01_bronze_layer.ipynb â†’ bronze_compras, bronze_detalles
3. Run 01_bronze_calidad.ipynb â†’ validaciones + parÃ¡metros
4. IF estado="Validacion Exitosa":
     Run 02_silver_layer.ipynb â†’ silver_compras, silver_detalles
   ELSE:
     Stop (enviar email de error)
5. Run 02_silver_calidad.ipynb â†’ validaciones
6. IF estado="Validacion Exitosa":
     Run 03_gold_layer.ipynb â†’ gold_fact_compras
   ELSE:
     Stop (enviar email de alert)
7. Run send_success.ipynb â†’ email notificaciÃ³n

EJECUCIÃ“N AUTOMATIZADA (con Job):
Databricks Workflow â†’ Todas las tareas arriba + condicionales + email
(Ver detalles en siguiente secciÃ³n)
```

---

## âœ… Validaciones de Calidad

### Bronze Layer Checks
```
âœ“ No nulls en columnas clave (factura, fecha_orden)
âœ“ Formato mÃ­nimo (ej: factura â‰¥ 7 chars)
âœ“ DetecciÃ³n de duplicados
```

### Silver Layer Checks
```
âœ“ Integridad referencial (fechas coherentes)
âœ“ Rangos vÃ¡lidos (ej: subtotal > 0)
```

### Gold Layer
```
âœ“ Completitud del join (100% de registros coinciden)
âœ“ Sin nulos en columnas analÃ­ticas clave
```

---

## ğŸ”§ AutomatizaciÃ³n con Databricks Jobs

**Nombre del Job**: `Pipeline ELT Compras`

### Estructura del Workflow

```
START
  â”‚
  â”œâ”€â†’ [Task] 00_Setup
  â”‚   â””â”€â†’ Status: Success âœ“
  â”‚
  â”œâ”€â†’ [Task] 01_Bronze_Layer
  â”‚   â””â”€â†’ Status: Success âœ“
  â”‚
  â”œâ”€â†’ [Task] 01_Bronze_Calidad
  â”‚   â”œâ”€â†’ IF estado="Validacion Exitosa": Continue
  â”‚   â””â”€â†’ ELSE: Send Email Error â†’ END
  â”‚
  â”œâ”€â†’ [Task] 02_Silver_Layer
  â”‚   â””â”€â†’ Status: Success âœ“
  â”‚
  â”œâ”€â†’ [Task] 02_Silver_Calidad
  â”‚   â”œâ”€â†’ IF estado="Validacion Exitosa": Continue
  â”‚   â””â”€â†’ ELSE: Send Email Quality Alert â†’ END
  â”‚
  â”œâ”€â†’ [Task] 03_Gold_Layer
  â”‚   â””â”€â†’ Status: Success âœ“
  â”‚
  â””â”€â†’ [Task] Send_Success_Email
      â””â”€â†’ END (Success)
```

### ConfiguraciÃ³n del Job

```bash
# Crear job con CLI:
databricks jobs create --json '{
  "name": "Pipeline ELT Compras",
  "tasks": [
    {"task_key": "setup", "notebook_task": {"notebook_path": "/Repos/..."}},
    {"task_key": "bronze_layer", "depends_on": [{"task_key": "setup"}], ...},
    ...
  ],
  "schedule": {"quartz_cron_expression": "0 2 * * ? *"},  # 2 AM daily
  "timeout_seconds": 3600
}'

# Ejecutar manualmente:
databricks jobs run-now --job-id 12345

# Ver ejecuciones:
databricks runs list --job-id 12345
```

### Email Notifications

Todos los tasks envÃ­an emails con parÃ¡metro `send_to`:

```python
# En cada task de email:
dbutils.widgets.text("send_to", "admin@company.com")
send_to = dbutils.widgets.get("send_to")

# Ejemplo: send_success.ipynb
smtplib.send(
  subject="âœ… Pipeline ELT Compras - EjecuciÃ³n Exitosa",
  body=f"Tablas actualizadas: bronze, silver, gold. Registros procesados: X"
)
```

---

## ğŸ“Š Resultados & MÃ©tricas

### Performance (Estimado con 50M registros)

| MÃ©trica | Valor | ObservaciÃ³n |
|---------|-------|-----------|
| **Ingesta Bronze** | ~2 min | CSV presencial + JSON online |
| **TransformaciÃ³n Silver** | ~4 min | Limpieza y tipo conversiÃ³n |
| **ValidaciÃ³n Calidad** | ~1.5 min | Reglas complejas |
| **Join & Gold** | ~3 min | 1 fact table de 100M+ filas |
| **Tiempo Total** | ~10.5 min | End-to-end pipeline |
| **Email Notification** | ~30 seg | Async |

### Calidad de Datos (Ejemplo)

```
ğŸ“Š REPORTE DE CALIDAD - EjecuciÃ³n 2025-06-16
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

BRONZE LAYER:
âœ… bronze_compras: 2,500,000 registros
   - Duplicados detectados: 250 (0.01%) â†’ Removidos
   - Nulos [factura]: 0 âœ“
   - Nulos [fecha_orden]: 0 âœ“
   
âœ… bronze_detalles: 5,000,000 registros
   - Nulos [factura]: 0 âœ“
   - Nulos [producto]: 0 âœ“

SILVER LAYER:
âœ… silver_compras: 2,500,000 registros
   - Inconsistencia fecha_envio: 15 registros (estado=Entregado pero fecha_envio=null)
   - Regla fallida: [fecha_envio] â‰¥ [fecha_orden]: 3 registros
   - Estado: âš ï¸ VALIDACION CON ADVERTENCIAS

âœ… silver_detalles: 5,000,000 registros
   - Subtotal < 0: 0 registros âœ“

GOLD LAYER:
âœ… gold_fact_compras: 5,000,000 registros
   - Join completitud: 100% âœ“
   - Ready for BI âœ…
```

## ğŸ“š Recursos Ãštiles

- ğŸ”— [Databricks Documentation](https://docs.databricks.com/)
- ğŸ”— [Delta Lake Guide](https://delta.io/learn/delta-lake-tutorial/)
- ğŸ”— [PySpark SQL API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/)
- ğŸ”— [Databricks Workflows](https://docs.databricks.com/workflows/)
- ğŸ”— [Data Quality Best Practices](https://docs.databricks.com/delta/tutorial/)

---

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas:

1. Fork el proyecto
2. Crear rama feature (`git checkout -b feature/MejorValidacion`)
3. Commit (`git commit -m 'Add nueva regla de calidad'`)
4. Push (`git push origin feature/MejorValidacion`)
5. Pull Request

---

## ğŸ“„ Licencia

MIT License - Ver [LICENSE](LICENSE)

---

## ğŸ‘¤ Contacto & Links

- ğŸ’¼ **LinkedIn**: [Tu Perfil](https://www.linkedin.com/in/geiby-maribel-sosa-chamba-300831b7/)
- ğŸŒ **Portfolio**: [Tu Web](https://portfolio-data-engineer.lovable.app/)
- ğŸ“§ **Email**: geibymari@gmail.com
- ğŸ“¹ **Video del Proyecto**: [LinkedIn Post](https://www.linkedin.com/posts/geiby-maribel-sosa-chamba-300831b7_dataengineering-databricks-elt-activity-7378228733519876096-FNgJ?utm_source=share&utm_medium=member_desktop&rcm=ACoAABjci3ABc4Wz2UXPphN0W0ty6TIVmuuEbsI)

---

## ğŸ”” Changelog

**v1.0.0** (2025-11-17)
- âœ… Pipeline ELT completo (Bronze â†’ Silver â†’ Gold)
- âœ… Validaciones de calidad en 3 capas
- âœ… AutomatizaciÃ³n con Databricks Jobs
- âœ… Notificaciones por email
- âœ… DocumentaciÃ³n completa

---

**Ãšltima actualizaciÃ³n**: Noviembre 2025  
**VersiÃ³n**: 1.0.0  
**Estado**: âœ… Production Ready

â­ **Si te fue Ãºtil, dale una estrella en GitHub!** â­
